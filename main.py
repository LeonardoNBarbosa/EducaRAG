import os
import streamlit as st
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core import StorageContext
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.groq import Groq
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client

# Configura칞칫es da p치gina com streamlit
st.set_page_config(
    page_title="EduRAG", 
    page_icon="游불",
    layout="centered",
    initial_sidebar_state="auto",
    menu_items=None
)

st.title("游불 EduRAG")
st.info("O assistente ir치 ajuda-lo a gerar PEIs (Plano de Ensino Individualizado) para seus alunos de forma r치pida e eficiente.")

# Defini칞칚o das chaves API
groq_chave = st.secrets["GROQ_CHAVE"]
qdrant_chave = st.secrets["QDRANT_CHAVE"]

# Cria o chat e o inicializa com uma mensagem
if "mensagens" not in st.session_state.keys():
    st.session_state.mensagens = [
        {"role": "assistant", "content": "Ol치! Qual o plano para hoje?"}
    ]

def conectar_qdrant():
    client = qdrant_client.QdrantClient(
        url="https://ab974ce8-55d9-44d7-9250-eff6f00333e0.sa-east-1-0.aws.cloud.qdrant.io",
        api_key=qdrant_chave,
        check_compatibility=False,
    )
    return client


# TODO colocar sistema de prompts
Settings.llm = Groq(
    model="llama-3.3-70b-versatile",
    api_key=groq_chave
)

Settings.embed_model = HuggingFaceEmbedding( 
    model_name="BAAI/bge-m3",
    device="cpu",
    embed_batch_size=10,
    model_kwargs={
        "trust_remote_code": True,
        "low_cpu_mem_usage": False
    }
)

def primeiro_carregamento():
    documentos = SimpleDirectoryReader(input_dir="data/pdfs/").load_data() # Verificar sobre quantidade de chunks

    client = conectar_qdrant()
    
    # Definindo o vector store para armazenar os dados indexados
    vector_store = QdrantVectorStore(collection_name="EducaRAG-v2", client=client)

    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    index = VectorStoreIndex.from_documents(documentos, storage_context=storage_context, embed_model=Settings.embed_model)
    return index

# Buscando os dados no Qdrant Cloud, depois dos documentos j치 indexados
@st.cache_resource(show_spinner=False) # TODO: documentar o que faz
def carregamento_definitivo():
    client = conectar_qdrant()

    # Altera칞칚o dos modelos de LLM e embedding do llama-index
    # TODO: Testar outras LLMs e embed models para verificar melhora nas respostas
    # TODO: Verificar temperatura e max tokens

    vector_store = QdrantVectorStore(collection_name="EducaRAG-v2", client=client)

    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=Settings.embed_model)
    return index


index = carregamento_definitivo()

# Inicializa o chat engine com st_session_state
if "chat_engine" not in st.session_state.keys():
    # TODO: testar chat engine com outras parametriza칞칫es
    st.session_state.chat_engine = index.as_chat_engine(
        chat_mode="condense_question", 
        verbose=True,
        streaming=False,
        llm=Settings.llm
    )

# Prompt para inser칞칚o do usu치rio e salva no hist칩rico
if prompt := st.chat_input(
    "Digite aqui"
): 
    st.session_state.mensagens.append({"role": "user", "content": prompt})

for mensagem in st.session_state.mensagens:
    with st.chat_message(mensagem["role"]):
        st.write(mensagem["content"])

# Se a 칰ltima mensagem n칚o for do assistente, gere uma nova resposta
if st.session_state.mensagens[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            resposta_stream = st.session_state.chat_engine.stream_chat(prompt)

            st.write_stream(resposta_stream.response_gen)

            mensagem = {"role": "assistant", "content": resposta_stream.response}

            st.session_state.mensagens.append(mensagem)
else:
    pass